{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# temporary solution for relative imports in case pyod is not installed\n",
    "# if pyod is installed, no need to use the following line\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from sklearn.metrics import average_precision_score\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of inliers and outliers\n",
    "n_samples = 256\n",
    "outliers_fraction = 0.25\n",
    "clusters_separation = [0]\n",
    "\n",
    "# Compare given detectors under given settings\n",
    "# Initialize the data\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\n",
    "n_inliers = int((1. - outliers_fraction) * n_samples)\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "ground_truth = np.zeros(n_samples, dtype=int)\n",
    "ground_truth[-n_outliers:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "# Define nine outlier detection tools to be compared\n",
    "classifiers = {\n",
    "    'Isolation Forest': IsolationForest(n_estimators= 100, \n",
    "                                random_state=random_state, max_samples=256, bootstrap = False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (214). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit the models with the generated data and \n",
    "# compare model performances\n",
    "for i, offset in enumerate(clusters_separation):\n",
    "    np.random.seed(42)\n",
    "    # Data generation\n",
    "    #X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset\n",
    "    #X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset\n",
    "    #X = np.r_[X1, X2]\n",
    "    # Add outliers\n",
    "    #X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]\n",
    "    data = scipy.io.loadmat('C:\\\\Users\\\\anton\\\\OneDrive\\\\Skrivbord\\\\Thesis_Code\\\\IsolationForestTinyML\\\\DatSets\\\\glass.mat',\n",
    "                        squeeze_me=False)\n",
    "\n",
    "    enlist = list(data.items())\n",
    "    X = np.array(enlist, dtype=object)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 1, 0)\n",
    "    print(X)\n",
    "    y_true = np.empty([1,0], dtype=int)\n",
    "\n",
    "    #print(X[1][0][0])\n",
    "    k = 0\n",
    "    while k < len(X[1][0]):\n",
    "        y_true = np.append(y_true, int(X[1][0][k]))\n",
    "        k += 1\n",
    "\n",
    "    X = X[0][0]\n",
    "    \n",
    "   \n",
    "    # Fit the model\n",
    "    #plt.figure(figsize=(15, 12))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        #print(i + 1, 'fitting', clf_name)\n",
    "        # fit the data and tag outliers\n",
    "        p = 0\n",
    "        aps = []\n",
    "        #while p < 10:\n",
    "        clf.fit(X)\n",
    "        #WARNING put +1\n",
    "        scores_pred = clf.decision_function(X) * 1 \n",
    "        #threshold = percentile(scores_pred, 100 * outliers_fraction)\n",
    "        #y_pred = clf.predict(A) * -1\n",
    "        #y_pred = (y_pred + 1) / 2\n",
    "        #aps.append(average_precision_score(y_true, scores_pred))\n",
    "        #p += 1\n",
    "        #n_errors = (y_pred != ground_truth).sum()\n",
    "        # plot the levels lines and the points\n",
    "        #WARNING +1\n",
    "        #Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * 1\n",
    "        #Z = Z.reshape(xx.shape)\n",
    "        #plt.figure(figsize=[10,10])\n",
    "        #subplot = plt.subplot(2, 2, 1)\n",
    "        #subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "        #                 cmap=plt.cm.Blues_r)\n",
    "        #a = subplot.contour(xx, yy, Z, levels=[-threshold],\n",
    "        #                    linewidths=2, colors='red')\n",
    "        #subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "        #                 colors='orange')\n",
    "        #b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',\n",
    "        #                    s=20, edgecolor='k')\n",
    "        #c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',\n",
    "        #                    s=20, edgecolor='k')\n",
    "        #subplot.axis('tight')\n",
    "        #subplot.legend(\n",
    "         #   [a.collections[0], b, c],\n",
    "        #    ['learned decision function', 'true inliers', 'true outliers'],\n",
    "          #  prop=matplotlib.font_manager.FontProperties(size=10),\n",
    "          #  loc='lower right')\n",
    "        #subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n",
    "        #subplot.set_xlim((-7, 7))\n",
    "        #subplot.set_ylim((-7, 7))\n",
    "    #plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "    #plt.suptitle(\"Outlier detection\")\n",
    "#plt.show()\n",
    "#plt.boxplot(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"save_vectors.ino\", \"w\")\n",
    "\n",
    "#te(\"};\\n\\n\")\n",
    "\n",
    "#file.write(\"vector<vector<Tree>> iForest;\\n\")\n",
    "m = 0\n",
    "#while m < clf.n_estimators:\n",
    "    #file.write(\"std::vector<Tree> iTree\" + str(m + 1) + \";\\n\")\n",
    "    #m += 1\n",
    "#file.write(\"\\n\")\n",
    "\n",
    "f = 0\n",
    "#file.write(\"void setup() {\\n\")\n",
    "#file.write(\"    Serial.begin(9600);\\n\")\n",
    "while f < clf.n_estimators:\n",
    "    j = 0\n",
    "    while j < len(clf.estimators_[f].tree_.feature):\n",
    "        temp_child_l = clf.estimators_[f].tree_.children_left[j]\n",
    "        if clf.estimators_[f].tree_.children_left[j] == -1:\n",
    "            temp_child_l = 0\n",
    "\n",
    "        temp_child_r = clf.estimators_[f].tree_.children_right[j]\n",
    "        if temp_child_r == clf.estimators_[f].tree_.children_right[j] == -1:\n",
    "            temp_child_r = 0\n",
    "\n",
    "        temp_feature = clf.estimators_[f].tree_.feature[j]\n",
    "        temp_threshold = clf.estimators_[f].tree_.threshold[j]\n",
    "        temps_values = clf.estimators_[f].tree_.n_node_samples[j]\n",
    "\n",
    "        file.write(\"    iTree\" + str(f + 1) + \".push_back({\"  \n",
    "                                                            + str(temp_child_l) + \", \"\n",
    "                                                            + str(temp_child_r) + \", \"\n",
    "                                                            + str(temp_feature) + \", \"\n",
    "                                                            + str(temps_values) + \", \"\n",
    "                                                            + str(temp_threshold) + \n",
    "                                                        \"});\\n\")\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    file.write(\"    iForest.push_back(iTree\" + str(f + 1) + \");\\n\\n\")\n",
    "    f += 1\n",
    "\n",
    "rows = f*j\n",
    "memory_per_vector = 8\n",
    "allocated_memory = rows * memory_per_vector\n",
    "#file.write(\"}\\n\")\n",
    "#file.write(\"void loop() {\\n\")\n",
    "#file.write(\"\\n}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07560264966729513\n",
      "0.14398585908894307\n",
      "0.14106165540535753\n",
      "0.16140858737854608\n",
      "0.15904962771572811\n",
      "0.0903066613329987\n",
      "0.16020871384764535\n",
      "0.14539492532927906\n",
      "0.13878965380373354\n",
      "0.14755381679988888\n",
      "0.08793945803272535\n",
      "0.1611818721315783\n",
      "0.09637470037423229\n",
      "0.13104394769082056\n",
      "0.1526993333531288\n",
      "0.1572199581368941\n",
      "0.15124280596311218\n",
      "0.09389239001954125\n",
      "0.12454699589141038\n",
      "0.1347929061953822\n",
      "0.12946218732200893\n",
      "0.0495048165767108\n",
      "0.1625456564854887\n",
      "0.1649447192500414\n",
      "0.16093134469915665\n",
      "0.16244392305596989\n",
      "0.16494537130878079\n",
      "0.166092642742454\n",
      "0.14727329938368505\n",
      "0.1646132264747509\n",
      "0.1340121919703312\n",
      "0.1591940098569165\n",
      "0.10906601237376157\n",
      "0.12036257116553384\n",
      "0.15721010355735032\n",
      "0.14994307349594832\n",
      "0.11856183688537769\n",
      "0.15710598039249912\n",
      "0.09075311238764344\n",
      "0.09075311238764344\n",
      "0.1582886459459556\n",
      "0.1515630453073808\n",
      "0.16531351101583203\n",
      "0.10291900785309183\n",
      "0.09410814467589612\n",
      "0.1548516704006449\n",
      "0.13811956081542093\n",
      "0.010728551550779286\n",
      "0.1002520924935495\n",
      "0.1480300622389264\n",
      "0.02913861418350805\n",
      "0.1293346542628055\n",
      "0.15079601886344074\n",
      "0.15166436934003286\n",
      "0.1315656039935606\n",
      "0.06677982093062473\n",
      "0.03319906559922542\n",
      "0.16008739758097962\n",
      "0.15163581864386827\n",
      "0.1425420772478162\n",
      "0.14156706529264762\n",
      "0.05595020539705192\n",
      "0.08889778455308622\n",
      "0.09146147357597001\n",
      "0.09926901246521469\n",
      "0.12513249617273803\n",
      "0.09385868633805955\n",
      "0.0943856061832356\n",
      "0.09589921553554787\n",
      "0.07759317828679069\n",
      "0.03385889472322395\n",
      "0.05834550212438888\n",
      "0.14560412750776336\n",
      "0.15120804190165588\n",
      "0.14305206811512555\n",
      "0.14815877869479732\n",
      "0.15564254939475597\n",
      "0.15668707939496784\n",
      "0.09581988679929321\n",
      "0.1289148498270936\n",
      "0.1155321557285679\n",
      "0.14305575681422866\n",
      "0.15309017065154956\n",
      "0.13196801871141706\n",
      "0.02069968159097174\n",
      "0.15516905440899137\n",
      "0.13518068624253748\n",
      "0.14053330693997595\n",
      "0.15710514334150266\n",
      "0.1024849966144355\n",
      "0.10807450242233496\n",
      "0.14454031567323894\n",
      "0.08438337940017965\n",
      "0.14086153111629873\n",
      "0.1358999280490955\n",
      "0.1574147808457942\n",
      "0.12966186244427153\n",
      "0.06318206970769219\n",
      "0.12162607181450491\n",
      "0.1305803425655364\n",
      "0.08104365588768847\n",
      "0.11781726686596228\n",
      "0.03823266207569059\n",
      "-0.02696045315315565\n",
      "0.03784948984057229\n",
      "-0.11890878024430607\n",
      "-0.22440207819713276\n",
      "-0.16225811967184345\n",
      "0.026929605155273373\n",
      "-0.01898877643378516\n",
      "-0.04957994834173918\n",
      "-0.06270869569888671\n",
      "-0.04162080563174875\n",
      "0.11962618768266348\n",
      "0.14537298612875466\n",
      "0.1464608426596371\n",
      "0.1274936136134976\n",
      "0.11967025155602057\n",
      "0.08474133384797822\n",
      "0.15180593076868854\n",
      "0.15198925170238314\n",
      "0.11566783966784039\n",
      "0.15898121881152366\n",
      "0.14330244556924204\n",
      "0.12448047880043117\n",
      "0.13060636741520454\n",
      "0.16217128090434246\n",
      "0.06848105531959553\n",
      "0.034805250149076594\n",
      "0.0461676639064077\n",
      "0.05990924351750149\n",
      "-0.037503494927341285\n",
      "0.14478959585859508\n",
      "0.10019449381006618\n",
      "0.15104604828157003\n",
      "0.09299778348571675\n",
      "0.13125526372089302\n",
      "0.15646260305371845\n",
      "0.13785826173664983\n",
      "0.14782804330123672\n",
      "0.15071558244778255\n",
      "0.1095597386490903\n",
      "0.09894710612243018\n",
      "0.1450298504628268\n",
      "0.1204979532074748\n",
      "0.08377263970780191\n",
      "0.1362071244628828\n",
      "0.15744769276757575\n",
      "0.14839490264415856\n",
      "0.12966771185038228\n",
      "0.1168506164490063\n",
      "0.09041238684114344\n",
      "0.11071726789589181\n",
      "0.15454782857800164\n",
      "0.1594721043852641\n",
      "0.16032622626072485\n",
      "0.16110594464202183\n",
      "0.09832143474279204\n",
      "0.15171966934051268\n",
      "0.12642263919334945\n",
      "0.14821516225231096\n",
      "0.039116503475959685\n",
      "-0.004864543099951657\n",
      "-0.16546974841405826\n",
      "0.09130762019326122\n",
      "0.03632170242993865\n",
      "-0.0031678724379756718\n",
      "0.03434292330940225\n",
      "0.011715634393381469\n",
      "0.06480042797244623\n",
      "0.027677702997627507\n",
      "-0.15768883205637096\n",
      "-0.15676724793200414\n",
      "0.05016815595338986\n",
      "-0.09542348651938526\n",
      "-0.019321297353785627\n",
      "0.10256548285586298\n",
      "0.10975021669243712\n",
      "0.09201179996226366\n",
      "0.10202460874839453\n",
      "-0.027768700972683247\n",
      "0.05000976685449082\n",
      "0.05668237292565709\n",
      "0.02820659653741525\n",
      "-0.18488557746384449\n",
      "-0.09901167004099631\n",
      "-0.08068241310270807\n",
      "0.11710655393624303\n",
      "-0.03171367386786583\n",
      "-0.08965360327155214\n",
      "0.06282783331574499\n",
      "0.023730165978384565\n",
      "0.01121116261930466\n",
      "0.027227695885361614\n",
      "0.03410058075845751\n",
      "0.021598405909299956\n",
      "0.010141845848064268\n",
      "0.0655968175627824\n",
      "0.04482069864590471\n",
      "0.040304727022427156\n",
      "0.04048335803290842\n",
      "-0.08978099303064857\n",
      "0.033229645489602166\n",
      "0.04698562774415821\n",
      "0.06550597045111978\n",
      "0.056679497245106625\n",
      "0.056361714136081154\n",
      "-0.09521497710739402\n",
      "0.035994179612919064\n",
      "0.025263042707842298\n",
      "0.060837794314253624\n",
      "0.017071347241029045\n",
      "0.044418778421488614\n",
      "0.04984461293884579\n"
     ]
    }
   ],
   "source": [
    "for x in scores_pred:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n",
      "C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_iforest.py:289: UserWarning: max_samples (256) is greater than the total number of samples (129). max_samples will be set to n_samples for estimation.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02682674420412553\n",
      "0.003925251386135904\n",
      "0.055856001256652126\n",
      "-0.003240821756130535\n",
      "0.006645978457667618\n",
      "0.03714228016286762\n",
      "-0.0013619404627886079\n",
      "0.06700811272616286\n",
      "-0.03156230255705178\n",
      "0.006355089616520088\n",
      "-0.0556227324597729\n",
      "0.01621721816254686\n",
      "0.045564333272376106\n",
      "0.05697486290347048\n",
      "0.0384596832486903\n",
      "0.05977235040257865\n",
      "0.07916905223411541\n",
      "0.0006190391109765654\n",
      "0.06437090986307537\n",
      "0.018930477182975185\n",
      "-0.04363549987885114\n",
      "0.053512435549945336\n",
      "0.009203612563567349\n",
      "0.0793682816238569\n",
      "-0.05589349026560687\n",
      "0.03166315190044014\n",
      "0.05004953114544105\n",
      "0.04128453561704009\n",
      "0.07162527209018822\n",
      "0.0021068268096527125\n",
      "0.060947215124835356\n",
      "0.060547523016019615\n",
      "0.09560295367601344\n",
      "0.06986052895832796\n",
      "0.0715909564002059\n",
      "0.05848364775194555\n",
      "0.09533773326890548\n",
      "0.09473079656149902\n",
      "0.054278085848808756\n",
      "0.09595155579556168\n",
      "0.06117198243324262\n",
      "0.08627163561459705\n",
      "0.08682131807921473\n",
      "0.076105585518137\n",
      "0.0683991651957792\n",
      "0.05755551371216336\n",
      "-0.028000960760601525\n",
      "0.011718762789180449\n",
      "0.08441330753696887\n",
      "0.02287170823854441\n",
      "0.01579470092912282\n",
      "0.05866143810220959\n",
      "0.07977215278902314\n",
      "0.09759074964641264\n",
      "0.08381319150998928\n",
      "0.08854060896560342\n",
      "0.048741232835750305\n",
      "0.1019169152658255\n",
      "0.09779319129361645\n",
      "0.08603554000119933\n",
      "0.05104954205848508\n",
      "-0.041663352689203764\n",
      "0.08733040715598886\n",
      "0.030982581867655512\n",
      "0.07019814552587794\n",
      "0.08596343578874359\n",
      "-0.0010633393654855441\n",
      "0.08595567342566526\n",
      "0.09273546326315674\n",
      "0.036016984587134274\n",
      "0.06340324928040722\n",
      "0.06496110741040649\n",
      "-0.12397101845801985\n",
      "0.03624958649228813\n",
      "0.017424120596597092\n",
      "-0.015847175125990286\n",
      "0.09692520641295584\n",
      "0.07357223010619492\n",
      "0.034903118768908184\n",
      "0.0903898675867388\n",
      "0.07980320010759223\n",
      "0.04419439608361738\n",
      "0.07503789320680333\n",
      "0.054934394549729454\n",
      "0.054849949952665324\n",
      "0.055909352021957445\n",
      "0.071111245854268\n",
      "0.024472750488601504\n",
      "0.03002746821357983\n",
      "0.0728862033020005\n",
      "0.07588868671491544\n",
      "0.08061990323263551\n",
      "0.06254100439814207\n",
      "0.05613616256727152\n",
      "0.06960782152216204\n",
      "0.03485514948900608\n",
      "0.07191170874282442\n",
      "-0.03592488648910844\n",
      "0.06735365641996331\n",
      "0.09218862024131515\n",
      "0.050133093096130156\n",
      "0.010024991204821943\n",
      "0.019952794456542747\n",
      "0.012697266061032015\n",
      "0.03535367251303975\n",
      "0.05891930761425801\n",
      "0.04361339483892287\n",
      "0.06129100028036653\n",
      "0.02071863257350244\n",
      "-0.06263626937260577\n",
      "0.0003849780690363793\n",
      "0.06483317406124955\n",
      "0.07468162037155425\n",
      "0.08426869058569011\n",
      "0.09571263459653036\n",
      "0.07060042967016122\n",
      "0.056775301063564806\n",
      "0.05206153703199813\n",
      "0.08102549019535707\n",
      "0.05944786069704187\n",
      "0.013892007741766823\n",
      "0.07171507777782071\n",
      "0.04765632785982784\n",
      "0.06972130096009593\n",
      "0.05188386093238258\n",
      "0.08773441407845892\n",
      "0.040039171543797956\n",
      "0.054645278691557775\n",
      "0.030985627831944237\n"
     ]
    }
   ],
   "source": [
    "# Fit the models with the generated data and \n",
    "# compare model performances\n",
    "for i, offset in enumerate(clusters_separation):\n",
    "    np.random.seed(42)\n",
    "    # Data generation\n",
    "    #X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset\n",
    "    #X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset\n",
    "    #X = np.r_[X1, X2]\n",
    "    # Add outliers\n",
    "    #X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]\n",
    "    data = scipy.io.loadmat('C:\\\\Users\\\\anton\\\\OneDrive\\\\Skrivbord\\\\Thesis_Code\\\\IsolationForestTinyML\\\\DatSets\\\\wine.mat',\n",
    "                        squeeze_me=False)\n",
    "\n",
    "    enlist = list(data.items())\n",
    "    X = np.array(enlist, dtype=object)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 0, 0)\n",
    "    X = np.delete(X, 0, 1)\n",
    "\n",
    "    y_true = np.empty([1,0], dtype=int)\n",
    "\n",
    "    #print(X[1][0][0])\n",
    "    k = 0\n",
    "    while k < len(X[1][0]):\n",
    "        y_true = np.append(y_true, int(X[1][0][k]))\n",
    "        k += 1\n",
    "\n",
    "    X = X[0][0]\n",
    "    # Fit the model\n",
    "    #plt.figure(figsize=(15, 12))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        #print(i + 1, 'fitting', clf_name)\n",
    "        # fit the data and tag outliers\n",
    "        p = 0\n",
    "        aps = []\n",
    "        all_scores = []\n",
    "        while p < 10:\n",
    "            clf.fit(X)\n",
    "            #WARNING put +1\n",
    "            scores_pred = clf.decision_function(X) * 1 \n",
    "            #threshold = percentile(scores_pred, 100 * outliers_fraction)\n",
    "            #y_pred = clf.predict(X) * -1\n",
    "            #y_pred = (y_pred + 1) / 2\n",
    "            #aps.append(average_precision_score(y_true, scores_pred))\n",
    "            all_scores.append(scores_pred)\n",
    "            p += 1\n",
    "\n",
    "    r = 0\n",
    "    average_scores = []\n",
    "\n",
    "    while r < len(all_scores[0]):\n",
    "        c = 0\n",
    "        avg = 0\n",
    "        while c < len(all_scores):\n",
    "            avg += all_scores[c][r]/10\n",
    "            c += 1\n",
    "        average_scores.append(avg)\n",
    "        r += 1\n",
    "\n",
    "    for a in average_scores:\n",
    "        print(a)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22088ef271373e2cbf099a2df3479705078d4b70103bf40dba9301d6ee5dcef5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
